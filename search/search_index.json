{
    "docs": [
        {
            "location": "/", 
            "text": "HSR Notizen\n\n\nSpring Semester 2018\n\n\n\n\nCloud Solutions\n\n\nCompilerbau\n\n\nDeep Learning\n\n\n\n\nLinks\n\n\n\n\nMathJax Syntax\n\n\nMkDocs User-Guide\n\n\nMarkdown Cheatsheet", 
            "title": "Home"
        }, 
        {
            "location": "/#hsr-notizen", 
            "text": "", 
            "title": "HSR Notizen"
        }, 
        {
            "location": "/#spring-semester-2018", 
            "text": "Cloud Solutions  Compilerbau  Deep Learning", 
            "title": "Spring Semester 2018"
        }, 
        {
            "location": "/#links", 
            "text": "MathJax Syntax  MkDocs User-Guide  Markdown Cheatsheet", 
            "title": "Links"
        }, 
        {
            "location": "/playground/", 
            "text": "Markdown Playground\n\n\nDies ist etwas normaler Text mit etwas \nkursiver\n schrift und etwas \nbold\n schrift\n\n\nDieser Text ist \nmarkiert\n, hier sind emojis: \n \n \n\n\n\n\n1\n2\nDies ist Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text\nzweite Zeile\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n  \npublic\n \nclass\n \nTestClass\n()\n \n{\n\n    \nint\n \nvar\n \n=\n \n1\n;\n\n\n    \npublic\n \nmethod\n()\n \n{\n\n\n      \nreturn\n \nvar\n;\n\n\n    \n}\n\n\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\n\n\n\n\n1\n\n\netwas\n\n\nanderes\n\n\n\n\n\n\n2\n\n\nals\n\n\nhier\n\n\n\n\n\n\n\n\n\n\nNotiz\n\n\nTest Notiz mit etwas Text drin\n\n\n\n\nNote\nKlapp mich auf!\n\n\nDanger\n\n\nGefahr!\n\n\n\n\n\n\nWarning\n\n\nWarnung!\n\n\n\n\n\n\nSummary\n\n\nEine Zusammenfassung\n\n\n\n\n\n\nInfo\n\n\nEine Information\n\n\n\n\n\n\nTip\n\n\nEin Tip\n\n\n\n\n\n\nQuestion\n\n\nEine Frage\n\n\n\n\n\n\nBug\n\n\nEin Bug\n\n\n\n\n\n\nQuote\n\n\nEin Zitat\n\n\n\n\nInline Math: \n x_i^2 * \\frac{(n^2 * n) - 1}{\\Omega} \n\n\n\n\nDisplay Math:\n\n\\sum_{i=0}^n i^2\n\n\n\n\nGruppen mit \n{}\n: \n{10}^5\n\n\n\n\n\n\nA \\rightarrow B \n\n\n\\lim_{x\\to \\infty} \\sin x", 
            "title": "Playground"
        }, 
        {
            "location": "/playground/#markdown-playground", 
            "text": "Dies ist etwas normaler Text mit etwas  kursiver  schrift und etwas  bold  schrift  Dieser Text ist  markiert , hier sind emojis:        1\n2 Dies ist Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text ein Text\nzweite Zeile   1\n2\n3\n4\n5\n6    public   class   TestClass ()   { \n     int   var   =   1 ;       public   method ()   {         return   var ;       }     }       1  2  3      1  etwas  anderes    2  als  hier      Notiz  Test Notiz mit etwas Text drin   Note Klapp mich auf!  Danger  Gefahr!    Warning  Warnung!    Summary  Eine Zusammenfassung    Info  Eine Information    Tip  Ein Tip    Question  Eine Frage    Bug  Ein Bug    Quote  Ein Zitat   Inline Math:   x_i^2 * \\frac{(n^2 * n) - 1}{\\Omega}    Display Math: \\sum_{i=0}^n i^2   Gruppen mit  {} :  {10}^5    A \\rightarrow B   \\lim_{x\\to \\infty} \\sin x", 
            "title": "Markdown Playground"
        }, 
        {
            "location": "/clou/", 
            "text": "Cloud Solutions", 
            "title": "Index"
        }, 
        {
            "location": "/clou/#cloud-solutions", 
            "text": "", 
            "title": "Cloud Solutions"
        }, 
        {
            "location": "/clou/introduction/", 
            "text": "Introduction\n\n\nAnforderung an Cloud Provider\n\n\n\n\nAvailability\n\n\nOn Boarding (einfach zu bedienen)\n\n\nVerschiedene Interfaces (z.B. API, Browser)\n\n\nOn Demand: Nicht auf Installation warten m\u00fcssen\n\n\nSecurity-Zusicherungen\n\n\nScalability\n\n\nTooling (z.B. Monitoring)\n\n\nModular\n\n\nKein Vendor lock-in\n\n\n\n\nMeasurable: z.B. transparente Kosten\n\n\n\n\n\n\nNIST-Standard ist weniger spezifisch als \"OSSM\"\n\n\n\n\nz.B. nichts \u00fcber Monitoring, API-Access, ...", 
            "title": "Introduction"
        }, 
        {
            "location": "/clou/introduction/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/clou/introduction/#anforderung-an-cloud-provider", 
            "text": "Availability  On Boarding (einfach zu bedienen)  Verschiedene Interfaces (z.B. API, Browser)  On Demand: Nicht auf Installation warten m\u00fcssen  Security-Zusicherungen  Scalability  Tooling (z.B. Monitoring)  Modular  Kein Vendor lock-in   Measurable: z.B. transparente Kosten    NIST-Standard ist weniger spezifisch als \"OSSM\"   z.B. nichts \u00fcber Monitoring, API-Access, ...", 
            "title": "Anforderung an Cloud Provider"
        }, 
        {
            "location": "/clou/sla_eval/", 
            "text": "Patterns, SLA, Evaluation\n\n\nSLAs\n\n\n\n\nAmazon bietet 99.99% Verf\u00fcgbarkeit in ihrer SLA\n\n\nEin SLA definiert messbar die Offerings des Providers, und welche Konsequenzen bei Verletzen entstehen\n\n\nDiese Ziele (aus NFA) sind Service Level Objectives (SLO)\n\n\n\n\n\n\nCustomer Agreement: Vertragsbedingungen, z.B. K\u00fcndigungsfrist\n\n\nAcceptable Use Policies (AUPs): F\u00fcr was darf der Service verwendet werden (z.B. keine Malware hosten)\n\n\nEin SLO besteht aus dem Service, das Quality-Attribut und ein Threshold-Value\n\n\nEs wird auch festgehalten, zu wie viel Prozent ein SLo eingehalten werden muss\n\n\nbsp. : SLO ist \"Response time under 1 second\", dies muss zu 90% eingehalten werden\n\n\n\n\n\n\n\n\nCC Patterns\n\n\nEvaluationstechniken und -kriterien", 
            "title": "SLAs/Patterns"
        }, 
        {
            "location": "/clou/sla_eval/#patterns-sla-evaluation", 
            "text": "", 
            "title": "Patterns, SLA, Evaluation"
        }, 
        {
            "location": "/clou/sla_eval/#slas", 
            "text": "Amazon bietet 99.99% Verf\u00fcgbarkeit in ihrer SLA  Ein SLA definiert messbar die Offerings des Providers, und welche Konsequenzen bei Verletzen entstehen  Diese Ziele (aus NFA) sind Service Level Objectives (SLO)    Customer Agreement: Vertragsbedingungen, z.B. K\u00fcndigungsfrist  Acceptable Use Policies (AUPs): F\u00fcr was darf der Service verwendet werden (z.B. keine Malware hosten)  Ein SLO besteht aus dem Service, das Quality-Attribut und ein Threshold-Value  Es wird auch festgehalten, zu wie viel Prozent ein SLo eingehalten werden muss  bsp. : SLO ist \"Response time under 1 second\", dies muss zu 90% eingehalten werden", 
            "title": "SLAs"
        }, 
        {
            "location": "/clou/sla_eval/#cc-patterns", 
            "text": "", 
            "title": "CC Patterns"
        }, 
        {
            "location": "/clou/sla_eval/#evaluationstechniken-und-kriterien", 
            "text": "", 
            "title": "Evaluationstechniken und -kriterien"
        }, 
        {
            "location": "/combau/", 
            "text": "Compilerbau", 
            "title": "Index"
        }, 
        {
            "location": "/combau/#compilerbau", 
            "text": "", 
            "title": "Compilerbau"
        }, 
        {
            "location": "/combau/syntax/", 
            "text": "Syntax\n\n\nRappi# Syntax\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\nrappi_sharp \n=\n \n{\nclass_declaration\n}.\n\n\n\nclass_declaration \n=\n \n    \nclass\n \nclass_identifier \n[\n:\n \nclass_identifier\n]\n \n{\n \n{\nfield_declaration\n}\n \n}\n.\n\n\n\n//\n \nnot specified exactly\n,\n \nare numbers allowed\n?\n\n\nclass_identifier \n=\n \nuppercase_letter \n{\nletter\n}.\n\n\n\nuppercase_letter \n=\n \nA\n \n|\n \n.\n.\n \n|\n \nZ\n.\n\n\n\nlowercase_letter \n=\n \na\n \n|\n \n.\n.\n \n|\n \nz\n.\n\n\n\nletter \n=\n \nuppercase_letter \n|\n \nlowercase_letter\n.\n\n\n\nfield_declaration \n=\n \nvariable_declaration \n|\n \nmethod_declaration\n.\n\n\n\nvariable_declaration \n=\n \ntype variable_identifier \n;\n.\n\n\n\ntype \n=\n \n//\n \nTODO\n:\n \ncan be user defined or default type\n\n\n\n//\n \nnot specified exactly\n\n\nvariable_identifier \n=\n \nlowercase_letter \n{\nletter\n}.\n\n\n\nmethod_declaration \n=\n \n    \ntype method_identifier \n(\n \n[\nparameter_list\n]\n \n)\n \n{\n \n{\nstatement\n}\n \n}\n.\n\n\n\n//\n \nTODO separate to class identifier\n?\n\n\nmethod_identifer \n=\n \nclass_identifier\n.\n\n\n\nparameter_list \n=\n \nparameter \n{\n,\n \nparameter\n}.\n\n\n\nparameter \n=\n \ntype variable_identifier\n.\n\n\n\nstatement \n=\n \n      \n;\n\n    \n|\n \nvariable_declaration\n\n    \n|\n \nvariable_assignment\n\n    \n|\n \nif_statement\n\n    \n|\n \nwhile_statement\n\n    \n|\n \nmethod_call\n\n    \n|\n \nreturn_statement\n.\n\n\n\nvariable_assignment \n=\n \nvariable_identifier \n=\n \nexpression \n;\n.\n\n\n\nif_statement \n=\n \n    \nif\n \n(\n \nexpression \n)\n \n{\n \n{\nstatement\n}\n \n}\n \n{\nelse\n \n{\n \n{\nstatement\n}\n \n}\n}.\n\n\n\nwhile_statement \n=\n \n    \nwhile\n \n(\n \nexpression \n)\n \n{\n \n{\nstatement\n}\n \n}\n.\n\n\n\nmethod_call \n=\n \n    \n[\ndesignator \n.\n]\n \nmethod_identifier \n(\n \n[\nargument_list\n]\n \n)\n \n;\n.\n\n\n\nreturn_statement \n=\n \nreturn\n \n[\nexpression\n]\n \n;\n.\n\n\n\nargument_list \n=\n \nexpression \n{\n,\n \nexpression\n}.\n\n\n\nexpression \n=\n\n      \nconstant_value\n\n    \n|\n \ncomparison_expression\n\n    \n|\n \nlogical_expression\n\n\n\n//\n \nTODO\n:\n \nExtend expression definition", 
            "title": "Syntax"
        }, 
        {
            "location": "/combau/syntax/#syntax", 
            "text": "", 
            "title": "Syntax"
        }, 
        {
            "location": "/combau/syntax/#rappi-syntax", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63 rappi_sharp  =   { class_declaration }.  class_declaration  =  \n     class   class_identifier  [ :   class_identifier ]   {   { field_declaration }   } .  //   not specified exactly ,   are numbers allowed ?  class_identifier  =   uppercase_letter  { letter }.  uppercase_letter  =   A   |   . .   |   Z .  lowercase_letter  =   a   |   . .   |   z .  letter  =   uppercase_letter  |   lowercase_letter .  field_declaration  =   variable_declaration  |   method_declaration .  variable_declaration  =   type variable_identifier  ; .  type  =   //   TODO :   can be user defined or default type  //   not specified exactly  variable_identifier  =   lowercase_letter  { letter }.  method_declaration  =  \n     type method_identifier  (   [ parameter_list ]   )   {   { statement }   } .  //   TODO separate to class identifier ?  method_identifer  =   class_identifier .  parameter_list  =   parameter  { ,   parameter }.  parameter  =   type variable_identifier .  statement  =  \n       ; \n     |   variable_declaration \n     |   variable_assignment \n     |   if_statement \n     |   while_statement \n     |   method_call \n     |   return_statement .  variable_assignment  =   variable_identifier  =   expression  ; .  if_statement  =  \n     if   (   expression  )   {   { statement }   }   { else   {   { statement }   } }.  while_statement  =  \n     while   (   expression  )   {   { statement }   } .  method_call  =  \n     [ designator  . ]   method_identifier  (   [ argument_list ]   )   ; .  return_statement  =   return   [ expression ]   ; .  argument_list  =   expression  { ,   expression }.  expression  = \n       constant_value \n     |   comparison_expression \n     |   logical_expression  //   TODO :   Extend expression definition", 
            "title": "Rappi# Syntax"
        }, 
        {
            "location": "/combau/lexer/", 
            "text": "Lexer\n\n\nTrailing else problem\n\n\n\n\nNicht eindeutige Syntax\n\n\nJava umgeht das mit separat definiertem \"StatementNoShortIf\"\n\n\n\n\nLexikalische Analyse\n\n\n\n\nAnalyse von Terminalsymbolen zu Tokens\n\n\nToken = \"Terminalsymbol\"\n\n\n\n\n\n\nVon Charakter-Stream zu Token-Stream\n\n\nFasst mehrere Textzeichen zusammen zu Tokens\n\n\nEliminiert Whitespaces und Code-Kommentare\n\n\nWhitespace ist aber relevant, um Tokens zu trennen\n\n\nKommentare k\u00f6nnen z.B. f\u00fcr Refactoring-Tools relevant sein\n\n\n\n\n\n\nMerkt Position von Tokens im Code f\u00fcr Fehlermeldung und Debugging\n\n\n\n\nNutzer f\u00fcr Parser\n\n\n\n\nParser arbeitet nur mit Tokens\n\n\nLookahead pro Symbol (nur n\u00e4chstes Symbol)\n\n\n\n\nTokens\n\n\n\n\nKeywords sind reservierte Begriffe\n\n\nInterpunktionen wie \n;\n, \n{\n, \n]\n etc.\n\n\nIdentifiers sind \"custom\" tokens wie Variablennamen (die keine keywords sein d\u00fcrfen)\n\n\nReservierte typennamen und Werte (true, false, ...) nicht als fixe Tokens scannen, sondern als Identifier\n\n\n\n\nRegul\u00e4re Sprachen\n\n\n\n\nScanner / Lexer versteht nur regul\u00e4re Sprachen\n\n\nin EBNF ohne Rekursion m\u00f6glich = regul\u00e4r\n\n\n= als eine EBNF-Regel formulierbar\n\n\n\n\n\n\n\u00ccnteger = Digit [ Integer ]\n ist rekursiv, kann aber regul\u00e4r formuliert werden\n\n\nals \nInteger = Digit { Digit }.\n\n\nSyntax \u00e4quivalent\n\n\n\n\n\n\nScanner kann regul\u00e4re Sprachen, Parser kontextfreie Sprachen\n\n\nLexer ist endlicher Automat\n\n\n\n\n\n\nkontextsensitive Sprache wird begrenzt durch Semantic Checker abgedeckt\n\n\n\n\nMaximum Munch\n\n\n\n\nScanner absorbiert immer m\u00f6glichst viel\n\n\na.k.a. Regex \"greedy evaluation\"\n\n\n\n\n\n\nH\u00f6rt bei whitespace auf\n\n\n\n\nKommentare\n\n\n\n\nNicht schachtelbar -\n regul\u00e4re Ausdr\u00fccke\n\n\n\n\nTips f\u00fcr Scanner\n\n\n\n\nOverflow in Integers?\n\n\nAchtung: Min-value ist 1 \"gr\u00f6sser\" als max value!\n\n\nim Lexer nicht aufl\u00f6sbar, 1 zu gross erlauben\n\n\n\n\n\n\nNegative Zahlen als fix Token \n-\n und Integer scannen\n\n\nString-Scanner:\n\n\nEscaping beachten\n\n\nNewline beachten, String terminiert?\n\n\n\n\n\n\nEin \n/\n kann ein Kommentar oder ein Divide-Tag sein\n\n\nZeile / Position mitf\u00fchren (Position alleine reicht)\n\n\n\n\nFehlerbehandlung\n\n\n\n\nError-Tokens zur\u00fcck liefern, damit nicht direkt beim ersten Fehler abgebrochen wird\n\n\nBraucht aber eine Korrektur", 
            "title": "Lexer"
        }, 
        {
            "location": "/combau/lexer/#lexer", 
            "text": "", 
            "title": "Lexer"
        }, 
        {
            "location": "/combau/lexer/#trailing-else-problem", 
            "text": "Nicht eindeutige Syntax  Java umgeht das mit separat definiertem \"StatementNoShortIf\"", 
            "title": "Trailing else problem"
        }, 
        {
            "location": "/combau/lexer/#lexikalische-analyse", 
            "text": "Analyse von Terminalsymbolen zu Tokens  Token = \"Terminalsymbol\"    Von Charakter-Stream zu Token-Stream  Fasst mehrere Textzeichen zusammen zu Tokens  Eliminiert Whitespaces und Code-Kommentare  Whitespace ist aber relevant, um Tokens zu trennen  Kommentare k\u00f6nnen z.B. f\u00fcr Refactoring-Tools relevant sein    Merkt Position von Tokens im Code f\u00fcr Fehlermeldung und Debugging", 
            "title": "Lexikalische Analyse"
        }, 
        {
            "location": "/combau/lexer/#nutzer-fur-parser", 
            "text": "Parser arbeitet nur mit Tokens  Lookahead pro Symbol (nur n\u00e4chstes Symbol)", 
            "title": "Nutzer f\u00fcr Parser"
        }, 
        {
            "location": "/combau/lexer/#tokens", 
            "text": "Keywords sind reservierte Begriffe  Interpunktionen wie  ; ,  { ,  ]  etc.  Identifiers sind \"custom\" tokens wie Variablennamen (die keine keywords sein d\u00fcrfen)  Reservierte typennamen und Werte (true, false, ...) nicht als fixe Tokens scannen, sondern als Identifier", 
            "title": "Tokens"
        }, 
        {
            "location": "/combau/lexer/#regulare-sprachen", 
            "text": "Scanner / Lexer versteht nur regul\u00e4re Sprachen  in EBNF ohne Rekursion m\u00f6glich = regul\u00e4r  = als eine EBNF-Regel formulierbar    \u00ccnteger = Digit [ Integer ]  ist rekursiv, kann aber regul\u00e4r formuliert werden  als  Integer = Digit { Digit }.  Syntax \u00e4quivalent    Scanner kann regul\u00e4re Sprachen, Parser kontextfreie Sprachen  Lexer ist endlicher Automat    kontextsensitive Sprache wird begrenzt durch Semantic Checker abgedeckt", 
            "title": "Regul\u00e4re Sprachen"
        }, 
        {
            "location": "/combau/lexer/#maximum-munch", 
            "text": "Scanner absorbiert immer m\u00f6glichst viel  a.k.a. Regex \"greedy evaluation\"    H\u00f6rt bei whitespace auf", 
            "title": "Maximum Munch"
        }, 
        {
            "location": "/combau/lexer/#kommentare", 
            "text": "Nicht schachtelbar -  regul\u00e4re Ausdr\u00fccke", 
            "title": "Kommentare"
        }, 
        {
            "location": "/combau/lexer/#tips-fur-scanner", 
            "text": "Overflow in Integers?  Achtung: Min-value ist 1 \"gr\u00f6sser\" als max value!  im Lexer nicht aufl\u00f6sbar, 1 zu gross erlauben    Negative Zahlen als fix Token  -  und Integer scannen  String-Scanner:  Escaping beachten  Newline beachten, String terminiert?    Ein  /  kann ein Kommentar oder ein Divide-Tag sein  Zeile / Position mitf\u00fchren (Position alleine reicht)", 
            "title": "Tips f\u00fcr Scanner"
        }, 
        {
            "location": "/combau/lexer/#fehlerbehandlung", 
            "text": "Error-Tokens zur\u00fcck liefern, damit nicht direkt beim ersten Fehler abgebrochen wird  Braucht aber eine Korrektur", 
            "title": "Fehlerbehandlung"
        }, 
        {
            "location": "/combau/parser/", 
            "text": "Parser\n\n\nQuiz letzte Woche\n\n\n\n\nLexer liefert nur den regul\u00e4ren Anteil\n\n\nKeine rekursive Syntax-Elemente, bzw. Elemente, die _nur rekursiv ausdruckbar sind\n\n\n\n\n\n\n\n\nTop-Down Parser\n\n\n\n\nInput: Terminalsymbole, Output: Syntaxbaum\n\n\nKann nur kontextfreie Sprache\n\n\nz.B. nicht, ob Variable deklariert ist, oder Parameter auf Argumente passen, type checking\n\n\nDiese sind kontextabh\u00e4ngig\n\n\n\n\n\n\nDer Parser muss eine Ableitung der Syntaxregeln finden, um einen gegebenen Input herzuleiten\n\n\nQuasi eine Ableitung r\u00fcckw\u00e4rts, von Token zu einer Regel\n\n\nDiese Woche: Syntax-Check, n\u00e4chste Woche Syntaxbaum\n\n\n\n\nSyntax-Baum\n\n\n\n\nKonkreter Syntax-Baum (Parse Tree) befolgt die Syntax genau\n\n\nAbstract Syntax-Tree (AST) kann unwichtige Details auslassen\n\n\nz.B. Klammern weg lassen\n\n\n\n\n\n\nAST kann nicht automatisch erzeugt werden, \"nach Gusto\" des Entwicklers\n\n\nSelber implementierte Parser k\u00f6nnen direkt AST liefern\n\n\n\n\nTop-Down vs. Bottom-Up\n\n\n\n\nTop-Down: Zuerst \"linke Seite\" der Regel anwenden, bis Eingabetext rauskommt\n\n\nImmer von links her aufl\u00f6sen\n\n\n\n\n\n\nBottom-Up: Mit Eingabetext beginnen, und Regeln darauf anwenden, bis man zum \"Startsymbol\" kommt\n\n\nUmgekehrt zum Top-Down Parsing\n\n\n\n\n\n\n\n\nRecursive Decent\n\n\n\n\nWir nutzen den Stack der Methodenaufrufe als \"Push-Down Automat\"\n\n\nPro Nicht-terminalsymbol eine Methode\n\n\nWenn ein Nicht-Terminalsymbol in Syntax (rechte Seite) vorkommt, wird die entsprechende Methode aufgerufen\n\n\nMethoden k\u00f6nnen sich gegenseitig rekursiv aufrufen\n\n\nGibt keine Entscheidungsprobleme bei Top-Down (zielorientiert)\n\n\nAnderer Ansatz, wenn das nicht geht: Produktion ausprobieren und bei Fehler \"backtracken\" (exponentielle Laufzeit)\n\n\n\n\n\n\n\n\nTools und Implementation\n\n\n\n\nOne Token lookahead reicht aus\n\n\nMomentan geben die Methoden \nvoid\n zur\u00fcck, wo sp\u00e4ter ein Syntaxbaum erstellt wird\n\n\nNur Syntax-Check\n\n\n\n\n\n\nF\u00fcr nicht-terminal-Symbole braucht es z.T. ein weiteres Lookahead\n\n\nM\u00f6gliche Terminalsymbole erfassen, die in einem nicht-terminal-symbol als erstes vorkommen k\u00f6nnen: \nFIRST\n Set\n\n\nFIRST-Sets k\u00f6nnen Vereinigungen von \"untergeordneten\" NT-Symbolen haben\n\n\nWenn zwei NT-Symbole nicht-disjunkte FIRST-Mengen haben, reicht ein One-Token-Lookahead nicht mehr aus, da nicht entschieden werden kann\n\n\nBraucht weiteres Lookahead, dann irgendwann drei, etc.\n\n\nStattdessen Syntax \"umformen\" f\u00fcr Parser\n\n\nF\u00fcr beide Regeln eine eigene Regeln erfassen und das Gemeinsame zusammen ziehen\n\n\n\n\n\n\n\n\n\n\n\n\nLinksrekursion\n\n\n\n\nz.B. \nSequence = Sequence [ Statement ]\n\n\nGibt bei Implementation bei Recursive Decent endlose Rekursion\n\n\nBottom-Up Parser k\u00f6nnen damit umgehen\n\n\nUmschreiben nach \nSequence = { Statement }\n\n\n\n\nSyntaxbaum\n\n\n\n\nStatement-Block ist ein Spezialfall von Statement, in anderen Sprachen ist ein Statement-Block auch ein Statement\n\n\nAchten auf Assoziativit\u00e4t bei Expressions\n\n\n\n\n\n\n\n\n\n\nFehlerbehandlung\n\n\n\n\nNur h\u00e4ufige Fehler korrigieren\n\n\nBasiert immer auf Hypothese, es k\u00f6nnen leicht Folgefehler entstehen\n\n\nAndere Fehler wie inkompatible Typen, falsche Argumentliste, etc. werden im Semantic Checker behandelt\n\n\n\n\nAttributierte Grammatiken\n\n\n\n\nEBNF-Grammatiken mit kontext-sensitiven Attributen erg\u00e4nzen\n\n\nSemantische Checks oder Erzeugen des AST\n\n\nWrite()\n ist eine Aktion und gibt lediglich etwas auf die Konsole aus\n\n\nAusgabe Folie 22: `1 2 - 3 4 - +\n\n\nPostfix-Notation wie bei HP Taschenrechner\n\n\n\n\n\n\nDie Aktionen werden immer zum Schluss \"der Parse-Methode\" ausgef\u00fchrt\n\n\nDer Parser kann auch direkt statische Ausdr\u00fccke evaluieren\n\n\nDirekt Syntaxbaum erzeugen mit \nnew BinaryExpression(...)\n etc. direkt im Attribut\n\n\nS-attributiert (synthetisiert): Attribute lesen nur Parameter von Teilregeln (rechte Seite)\n\n\nL-attributiert (Ererbt): Attribut kann auch \"linke\" Seite lesen\n\n\nEs kann alles in Grammatik beschrieben werden, in der Praxis ist dies aber un\u00fcbersichtlich und kompliziert\n\n\n\n\nBottom-Up Parser (LR-Parser)\n\n\n\n\nAnsatz: Tokenstream nehmen und \"zusammenfalten\" zu Syntaxbaum\n\n\nNach jedem gelesenen Symbol pr\u00fcfen, ob die Folge einer Regel entspricht\n\n\nWenn ja, \"Reduce\"\n\n\nWenn nein, das n\u00e4chste Zeichen auslesen (shift)\n\n\n\n\n\n\nAm Schluss muss das Startsymbol \u00fcbrig bleiben, sonst Syntaxfehler\n\n\nEs kann zu Entscheidungsschwierigkeiten kommen: \"Shift-reduce\" / \"Reduce-Reduce\" Probleme\n\n\nGrammatik anpassen oder \"von Hand\"\n\n\n\n\n\n\nLR ist \"m\u00e4chtiger\" als LL-Parser\n\n\nz.B. Linksrekursion", 
            "title": "Parser"
        }, 
        {
            "location": "/combau/parser/#parser", 
            "text": "", 
            "title": "Parser"
        }, 
        {
            "location": "/combau/parser/#quiz-letzte-woche", 
            "text": "Lexer liefert nur den regul\u00e4ren Anteil  Keine rekursive Syntax-Elemente, bzw. Elemente, die _nur rekursiv ausdruckbar sind", 
            "title": "Quiz letzte Woche"
        }, 
        {
            "location": "/combau/parser/#top-down-parser", 
            "text": "Input: Terminalsymbole, Output: Syntaxbaum  Kann nur kontextfreie Sprache  z.B. nicht, ob Variable deklariert ist, oder Parameter auf Argumente passen, type checking  Diese sind kontextabh\u00e4ngig    Der Parser muss eine Ableitung der Syntaxregeln finden, um einen gegebenen Input herzuleiten  Quasi eine Ableitung r\u00fcckw\u00e4rts, von Token zu einer Regel  Diese Woche: Syntax-Check, n\u00e4chste Woche Syntaxbaum", 
            "title": "Top-Down Parser"
        }, 
        {
            "location": "/combau/parser/#syntax-baum", 
            "text": "Konkreter Syntax-Baum (Parse Tree) befolgt die Syntax genau  Abstract Syntax-Tree (AST) kann unwichtige Details auslassen  z.B. Klammern weg lassen    AST kann nicht automatisch erzeugt werden, \"nach Gusto\" des Entwicklers  Selber implementierte Parser k\u00f6nnen direkt AST liefern", 
            "title": "Syntax-Baum"
        }, 
        {
            "location": "/combau/parser/#top-down-vs-bottom-up", 
            "text": "Top-Down: Zuerst \"linke Seite\" der Regel anwenden, bis Eingabetext rauskommt  Immer von links her aufl\u00f6sen    Bottom-Up: Mit Eingabetext beginnen, und Regeln darauf anwenden, bis man zum \"Startsymbol\" kommt  Umgekehrt zum Top-Down Parsing", 
            "title": "Top-Down vs. Bottom-Up"
        }, 
        {
            "location": "/combau/parser/#recursive-decent", 
            "text": "Wir nutzen den Stack der Methodenaufrufe als \"Push-Down Automat\"  Pro Nicht-terminalsymbol eine Methode  Wenn ein Nicht-Terminalsymbol in Syntax (rechte Seite) vorkommt, wird die entsprechende Methode aufgerufen  Methoden k\u00f6nnen sich gegenseitig rekursiv aufrufen  Gibt keine Entscheidungsprobleme bei Top-Down (zielorientiert)  Anderer Ansatz, wenn das nicht geht: Produktion ausprobieren und bei Fehler \"backtracken\" (exponentielle Laufzeit)", 
            "title": "Recursive Decent"
        }, 
        {
            "location": "/combau/parser/#tools-und-implementation", 
            "text": "One Token lookahead reicht aus  Momentan geben die Methoden  void  zur\u00fcck, wo sp\u00e4ter ein Syntaxbaum erstellt wird  Nur Syntax-Check    F\u00fcr nicht-terminal-Symbole braucht es z.T. ein weiteres Lookahead  M\u00f6gliche Terminalsymbole erfassen, die in einem nicht-terminal-symbol als erstes vorkommen k\u00f6nnen:  FIRST  Set  FIRST-Sets k\u00f6nnen Vereinigungen von \"untergeordneten\" NT-Symbolen haben  Wenn zwei NT-Symbole nicht-disjunkte FIRST-Mengen haben, reicht ein One-Token-Lookahead nicht mehr aus, da nicht entschieden werden kann  Braucht weiteres Lookahead, dann irgendwann drei, etc.  Stattdessen Syntax \"umformen\" f\u00fcr Parser  F\u00fcr beide Regeln eine eigene Regeln erfassen und das Gemeinsame zusammen ziehen", 
            "title": "Tools und Implementation"
        }, 
        {
            "location": "/combau/parser/#linksrekursion", 
            "text": "z.B.  Sequence = Sequence [ Statement ]  Gibt bei Implementation bei Recursive Decent endlose Rekursion  Bottom-Up Parser k\u00f6nnen damit umgehen  Umschreiben nach  Sequence = { Statement }", 
            "title": "Linksrekursion"
        }, 
        {
            "location": "/combau/parser/#syntaxbaum", 
            "text": "Statement-Block ist ein Spezialfall von Statement, in anderen Sprachen ist ein Statement-Block auch ein Statement  Achten auf Assoziativit\u00e4t bei Expressions", 
            "title": "Syntaxbaum"
        }, 
        {
            "location": "/combau/parser/#fehlerbehandlung", 
            "text": "Nur h\u00e4ufige Fehler korrigieren  Basiert immer auf Hypothese, es k\u00f6nnen leicht Folgefehler entstehen  Andere Fehler wie inkompatible Typen, falsche Argumentliste, etc. werden im Semantic Checker behandelt", 
            "title": "Fehlerbehandlung"
        }, 
        {
            "location": "/combau/parser/#attributierte-grammatiken", 
            "text": "EBNF-Grammatiken mit kontext-sensitiven Attributen erg\u00e4nzen  Semantische Checks oder Erzeugen des AST  Write()  ist eine Aktion und gibt lediglich etwas auf die Konsole aus  Ausgabe Folie 22: `1 2 - 3 4 - +  Postfix-Notation wie bei HP Taschenrechner    Die Aktionen werden immer zum Schluss \"der Parse-Methode\" ausgef\u00fchrt  Der Parser kann auch direkt statische Ausdr\u00fccke evaluieren  Direkt Syntaxbaum erzeugen mit  new BinaryExpression(...)  etc. direkt im Attribut  S-attributiert (synthetisiert): Attribute lesen nur Parameter von Teilregeln (rechte Seite)  L-attributiert (Ererbt): Attribut kann auch \"linke\" Seite lesen  Es kann alles in Grammatik beschrieben werden, in der Praxis ist dies aber un\u00fcbersichtlich und kompliziert", 
            "title": "Attributierte Grammatiken"
        }, 
        {
            "location": "/combau/parser/#bottom-up-parser-lr-parser", 
            "text": "Ansatz: Tokenstream nehmen und \"zusammenfalten\" zu Syntaxbaum  Nach jedem gelesenen Symbol pr\u00fcfen, ob die Folge einer Regel entspricht  Wenn ja, \"Reduce\"  Wenn nein, das n\u00e4chste Zeichen auslesen (shift)    Am Schluss muss das Startsymbol \u00fcbrig bleiben, sonst Syntaxfehler  Es kann zu Entscheidungsschwierigkeiten kommen: \"Shift-reduce\" / \"Reduce-Reduce\" Probleme  Grammatik anpassen oder \"von Hand\"    LR ist \"m\u00e4chtiger\" als LL-Parser  z.B. Linksrekursion", 
            "title": "Bottom-Up Parser (LR-Parser)"
        }, 
        {
            "location": "/combau/semantic_analysis/", 
            "text": "Semantische Analyse\n\n\nQuiz\n\n\n\n\nIst syntaktisch korrekt, weil ein \nif\n als condition eine Expression erwartet\n\n\nAber semantisch falsch, bool nicht addierbar etc.\n\n\nDies sind \nkontextsensitive\n Regeln\n\n\n\n\n\n\n\n\nSemantic Checker\n\n\n\n\nInput: AST oder konkreter Syntaxtree\n\n\nOutput: AST + Symboltabelle\n\n\nPr\u00fcfen, dass das Programm gem\u00e4ss Sprachregeln \"Sinn macht\"\n\n\nDeklarationen pr\u00fcfen (eindeutig und nicht mehrfach)\n\n\nTypen (Typregeln sind erf\u00fcllt)\n\n\nMethod-Calls (Argumente und Parameter sind kompatibel)\n\n\nUnd mehr...\n\n\n\n\n\n\nTransformiere Programm, so dass Code Generation einfach wird\n\n\nSyntaxbaum umstellen\n\n\nz.B. Vereinheitlichen von verschiedenen Loops, z.B. aus for- einen while-loop\n\n\nArray Boundary Check\n\n\nImplizite Casts\n\n\nusw.\n\n\n\n\n\n\n\n\nSymboltabelle\n\n\n\n\nDatenstruktur zur Verwaltung von Deklarationen\n\n\nZu jedem Namen ist der Scope angegeben\n\n\nScopes k\u00f6nnen rekursiv sein, z.B. Methode -\n Methode etc.\n\n\nScoping auch wichtig bei z.B. loops\n\n\n\n\nShadowing\n\n\n\n\n\u00dcberdecken von Deklarationen in \u00e4usserem Scope durch Defintion in innerem Scope\n\n\nIn Rappi-sharp m\u00f6glich\n\n\nUnterschied zu \"Hiding\": Beim Hiding verdecken Methoden/Variablen in einer Unterklasse die Deklaration in der Superklasse\n\n\nShadowing in \n\n\n\n\nGlobal Scope\n\n\n\n\nEs gibt einen global Scope\n\n\nBei uns k\u00f6nnen dort nur Klassen vorkommen\n\n\n\n\n\n\nKeywords wie \ntrue\n, \nint\n, \nbool\n k\u00f6nnen auch dort definiert werden\n\n\n\n\nDesign der Symboltabelle\n\n\n\n\nCompilationUnit\n ist der \"global\" Scope\n\n\nIm ersten Schritt werden die Deklarationen aufgel\u00f6st, erste danach werden Typen aufgel\u00f6st\n\n\nWeil w\u00e4hrend dem Durchlaufen Typen erst sp\u00e4ter deklariert werden k\u00f6nnten\n\n\n\n\n\n\nZu einer lokalen Variable wird dazu gespeichert, in welchen Statements die Variable \"lebt\"\n\n\nSp\u00e4testens jetzt muss gepr\u00fcft werden, dass \nint\n, \nbool\n etc nicht als Identifier verwendet werden!\n\n\nnull\n muss speziell behandelt werden\n\n\nEs gibt ein Dictionary, das Symbole zu Knoten-Referenzen mappt.\n\n\n\n\nVorgehen\n\n\n\n\nSymboltabelle aufbauen\n\n\nTypen aufl\u00f6sen\n\n\nAST durchlaufen und mit Symboltabelle verbinden\n\n\nAlle Vorkommenden Identifier verlinken mit entsprechendem Symbol \n\n\n\n\n\n\n(Unsere Aufgabe) Typen in AST bestimmen\n\n\n\n\nNamensaufl\u00f6sung\n\n\n\n\nz.B. Typen aufl\u00f6sen\n\n\nZuerst suche im innersten Scope, dann im n\u00e4chst \u00e4usseren Scope, usw. rekursiv\n\n\nSuche mit \nFind\n Method auf Symboltabelle\n\n\n\n\nType Checking\n\n\n\n\nImportant\n\n\nWichtig f\u00fcr \u00dcbung: Folie 32, Folie 36 Punkte 2, 4-7, 9, Folien 38-42\n\n\n\n\n\n\nF\u00fcr jede Expression den Typ bestimmen\n\n\nInsbesondere Binary/Binary Expressions: Typen checken\n\n\nAuch z.B. bei \nif\n: Ist condition ein bool?\n\n\n\n\nAblauf\n\n\n\n\nPost-Order-Traversierung\n\n\nZuerst child-nodes aufl\u00f6sen\n\n\n\n\n\n\nAST nicht ver\u00e4ndern, sondern z.B. Symboltabelle \nFixType()\n aufrufen\n\n\nNach dem Aufl\u00f6sen folgt das Checking\n\n\nz.B. Array length read-only, noch selbst implementieren\n\n\n\n\n\n\nBei Zuweisungen: Pr\u00fcfen, ob rechte und linke Seite gleichen Typ haben\n\n\nAusser bei Polymorphismus und \nnull\n\n\nGleiches Prinzip bei Methoden-Aufrufen\n\n\n\n\n\n\n\n\nStatische Analyse\n\n\n\n\nChecks wie Array-Bounds sind nicht allgemein entscheidbar\n\n\nKann vom Compiler transformiert werden, z.B. neue Statements einf\u00fcgen\n\n\n\n\n\n\nMuss bei einer Runtime aber sowieso \u00fcberpr\u00fcft werden (Sicherheit), deshalb lohnt es sich hier nicht", 
            "title": "Semantische Analyse"
        }, 
        {
            "location": "/combau/semantic_analysis/#semantische-analyse", 
            "text": "", 
            "title": "Semantische Analyse"
        }, 
        {
            "location": "/combau/semantic_analysis/#quiz", 
            "text": "Ist syntaktisch korrekt, weil ein  if  als condition eine Expression erwartet  Aber semantisch falsch, bool nicht addierbar etc.  Dies sind  kontextsensitive  Regeln", 
            "title": "Quiz"
        }, 
        {
            "location": "/combau/semantic_analysis/#semantic-checker", 
            "text": "Input: AST oder konkreter Syntaxtree  Output: AST + Symboltabelle  Pr\u00fcfen, dass das Programm gem\u00e4ss Sprachregeln \"Sinn macht\"  Deklarationen pr\u00fcfen (eindeutig und nicht mehrfach)  Typen (Typregeln sind erf\u00fcllt)  Method-Calls (Argumente und Parameter sind kompatibel)  Und mehr...    Transformiere Programm, so dass Code Generation einfach wird  Syntaxbaum umstellen  z.B. Vereinheitlichen von verschiedenen Loops, z.B. aus for- einen while-loop  Array Boundary Check  Implizite Casts  usw.", 
            "title": "Semantic Checker"
        }, 
        {
            "location": "/combau/semantic_analysis/#symboltabelle", 
            "text": "Datenstruktur zur Verwaltung von Deklarationen  Zu jedem Namen ist der Scope angegeben  Scopes k\u00f6nnen rekursiv sein, z.B. Methode -  Methode etc.  Scoping auch wichtig bei z.B. loops", 
            "title": "Symboltabelle"
        }, 
        {
            "location": "/combau/semantic_analysis/#shadowing", 
            "text": "\u00dcberdecken von Deklarationen in \u00e4usserem Scope durch Defintion in innerem Scope  In Rappi-sharp m\u00f6glich  Unterschied zu \"Hiding\": Beim Hiding verdecken Methoden/Variablen in einer Unterklasse die Deklaration in der Superklasse  Shadowing in", 
            "title": "Shadowing"
        }, 
        {
            "location": "/combau/semantic_analysis/#global-scope", 
            "text": "Es gibt einen global Scope  Bei uns k\u00f6nnen dort nur Klassen vorkommen    Keywords wie  true ,  int ,  bool  k\u00f6nnen auch dort definiert werden", 
            "title": "Global Scope"
        }, 
        {
            "location": "/combau/semantic_analysis/#design-der-symboltabelle", 
            "text": "CompilationUnit  ist der \"global\" Scope  Im ersten Schritt werden die Deklarationen aufgel\u00f6st, erste danach werden Typen aufgel\u00f6st  Weil w\u00e4hrend dem Durchlaufen Typen erst sp\u00e4ter deklariert werden k\u00f6nnten    Zu einer lokalen Variable wird dazu gespeichert, in welchen Statements die Variable \"lebt\"  Sp\u00e4testens jetzt muss gepr\u00fcft werden, dass  int ,  bool  etc nicht als Identifier verwendet werden!  null  muss speziell behandelt werden  Es gibt ein Dictionary, das Symbole zu Knoten-Referenzen mappt.", 
            "title": "Design der Symboltabelle"
        }, 
        {
            "location": "/combau/semantic_analysis/#vorgehen", 
            "text": "Symboltabelle aufbauen  Typen aufl\u00f6sen  AST durchlaufen und mit Symboltabelle verbinden  Alle Vorkommenden Identifier verlinken mit entsprechendem Symbol     (Unsere Aufgabe) Typen in AST bestimmen", 
            "title": "Vorgehen"
        }, 
        {
            "location": "/combau/semantic_analysis/#namensauflosung", 
            "text": "z.B. Typen aufl\u00f6sen  Zuerst suche im innersten Scope, dann im n\u00e4chst \u00e4usseren Scope, usw. rekursiv  Suche mit  Find  Method auf Symboltabelle", 
            "title": "Namensaufl\u00f6sung"
        }, 
        {
            "location": "/combau/semantic_analysis/#type-checking", 
            "text": "Important  Wichtig f\u00fcr \u00dcbung: Folie 32, Folie 36 Punkte 2, 4-7, 9, Folien 38-42    F\u00fcr jede Expression den Typ bestimmen  Insbesondere Binary/Binary Expressions: Typen checken  Auch z.B. bei  if : Ist condition ein bool?", 
            "title": "Type Checking"
        }, 
        {
            "location": "/combau/semantic_analysis/#ablauf", 
            "text": "Post-Order-Traversierung  Zuerst child-nodes aufl\u00f6sen    AST nicht ver\u00e4ndern, sondern z.B. Symboltabelle  FixType()  aufrufen  Nach dem Aufl\u00f6sen folgt das Checking  z.B. Array length read-only, noch selbst implementieren    Bei Zuweisungen: Pr\u00fcfen, ob rechte und linke Seite gleichen Typ haben  Ausser bei Polymorphismus und  null  Gleiches Prinzip bei Methoden-Aufrufen", 
            "title": "Ablauf"
        }, 
        {
            "location": "/combau/semantic_analysis/#statische-analyse", 
            "text": "Checks wie Array-Bounds sind nicht allgemein entscheidbar  Kann vom Compiler transformiert werden, z.B. neue Statements einf\u00fcgen    Muss bei einer Runtime aber sowieso \u00fcberpr\u00fcft werden (Sicherheit), deshalb lohnt es sich hier nicht", 
            "title": "Statische Analyse"
        }, 
        {
            "location": "/dl/", 
            "text": "Deep Learning", 
            "title": "Index"
        }, 
        {
            "location": "/dl/#deep-learning", 
            "text": "", 
            "title": "Deep Learning"
        }, 
        {
            "location": "/dl/linear_algebra/", 
            "text": "Linear Algebra\n\n\n\n\nIndices in Matrix: First index is row (\ni\n: row, \nj\n: column)\n\n\nA 3D (or higher-dimensional) array is a \ntensor\n\n\nIn DL, we add scalars to matrices (not in linearl algebra): \nD_{i,j} = a \\cdot B_{i,j} + c\n\n\n\n\n\n\nMatrix Multiplication\n\n\n\n\nDimensions must be (q,p) = (q,k)(k,p)\n\n\nk\n-sized vectors are multiplied together\n\n\n\n\n\n\nHadamard Prodcut\n: Element-wise product, dimensions must be the same\n\n\nMatrix multiplication is associative and distributive, but \nnot commutative\n\n\n\n\nAB \\neq BA\n\n\n\n\n\n\n\n\n\n\nInverse Matrix\n\n\n\n\nMatrix which when multiplied with A will result in the Identity Matrix\n\n\nUsed for solving equations\n\n\n\n\nAx = b \\Rightarrow x = A^{-1}b\n\n\n\n\nNumerically not ideal\n\n\n\n\n\n\nEquation system could also have no or infinite solutions\n\n\nNo \"multipled solutions\", because an infinite number of solutions can be built with two of them\n\n\n\n\n\n\nFor \nA^{-1}\n to exist, we need \nm = n\n and all columns must be \nlinearly independent\n\n\nmeaning no vector can be created by scaling two others\n\n\n\n\n\n\n\n\nNorms\n\n\n\n\nMeasure to measure the length of a vector\n\n\nMost \"famous\": Euclidian norm (\"pythagoras\")\n\n\nThe \nsquared\n euclidian norm can be calculates as \nx^Tx\n\n\n\n\n\n\nL^1\n-norm: Add up every absolute value of the vector\n\n\nBetter for values closer to zero\n\n\n\n\n\n\n\n\nL^0\n-norm: Count the number of non-zero entries\n\n\nnot a mathematical norm\n, properties of a norm don't hold\n\n\n\n\n\n\n\n\nL^\\infty\n is just the max\n\n\nFrobenius norm: Square every entry of a Matrix and square the result\n\n\n\n\nSpecial Matrices\n\n\n\n\nDiagonal Matrix: Everything 0 except diagonal\n\n\ndiag(v)\n: \nv\n is the vector formed by the diagonal\n\n\ndiag(v)^-1\n = `diag([1/v1, ... 1/vn])\n\n\nSymmetric matrix: \nA = A^{-1}\n\n\n\n\northogonal matrix: rows and columns are mutually orthonormal\n\n\ntwo vectors orthonomal =\n dot product = 0\n\n\n\n\n\n\n\n\nEigendecomposition\n\n\n\n\n\n\nAv = \\lambda v\n\n\n\n\nWe need to find Eigenvector \nv\n and Eigenvalue (\\lambda)\n\n\nWe usually scale the Eigenvector to have length 1 (euclidian norm)\n\n\n\n\nA = V \\text{diag}(\\lambda)V^{-1}", 
            "title": "Linear Algebra"
        }, 
        {
            "location": "/dl/linear_algebra/#linear-algebra", 
            "text": "Indices in Matrix: First index is row ( i : row,  j : column)  A 3D (or higher-dimensional) array is a  tensor  In DL, we add scalars to matrices (not in linearl algebra):  D_{i,j} = a \\cdot B_{i,j} + c", 
            "title": "Linear Algebra"
        }, 
        {
            "location": "/dl/linear_algebra/#matrix-multiplication", 
            "text": "Dimensions must be (q,p) = (q,k)(k,p)  k -sized vectors are multiplied together    Hadamard Prodcut : Element-wise product, dimensions must be the same  Matrix multiplication is associative and distributive, but  not commutative   AB \\neq BA", 
            "title": "Matrix Multiplication"
        }, 
        {
            "location": "/dl/linear_algebra/#inverse-matrix", 
            "text": "Matrix which when multiplied with A will result in the Identity Matrix  Used for solving equations   Ax = b \\Rightarrow x = A^{-1}b   Numerically not ideal    Equation system could also have no or infinite solutions  No \"multipled solutions\", because an infinite number of solutions can be built with two of them    For  A^{-1}  to exist, we need  m = n  and all columns must be  linearly independent  meaning no vector can be created by scaling two others", 
            "title": "Inverse Matrix"
        }, 
        {
            "location": "/dl/linear_algebra/#norms", 
            "text": "Measure to measure the length of a vector  Most \"famous\": Euclidian norm (\"pythagoras\")  The  squared  euclidian norm can be calculates as  x^Tx    L^1 -norm: Add up every absolute value of the vector  Better for values closer to zero     L^0 -norm: Count the number of non-zero entries  not a mathematical norm , properties of a norm don't hold     L^\\infty  is just the max  Frobenius norm: Square every entry of a Matrix and square the result", 
            "title": "Norms"
        }, 
        {
            "location": "/dl/linear_algebra/#special-matrices", 
            "text": "Diagonal Matrix: Everything 0 except diagonal  diag(v) :  v  is the vector formed by the diagonal  diag(v)^-1  = `diag([1/v1, ... 1/vn])  Symmetric matrix:  A = A^{-1}   orthogonal matrix: rows and columns are mutually orthonormal  two vectors orthonomal =  dot product = 0", 
            "title": "Special Matrices"
        }, 
        {
            "location": "/dl/linear_algebra/#eigendecomposition", 
            "text": "Av = \\lambda v   We need to find Eigenvector  v  and Eigenvalue (\\lambda)  We usually scale the Eigenvector to have length 1 (euclidian norm)   A = V \\text{diag}(\\lambda)V^{-1}", 
            "title": "Eigendecomposition"
        }, 
        {
            "location": "/dl/numerical_computation/", 
            "text": "Numerical Computation\n\n\n\n\nMost of this is handled by frameworks\n\n\n\n\nOverflow and Underflow\n\n\n\n\nUnderflow: So small that they're basically 0\n\n\n\n\n\\log x\n when x is zero: \n-\\infty\n\n\n\n\n\n\n\n\nsoftmax takes a vector, the sum will be 1, and every number will be positive -\n probability distribution\n\n\nsigmoid is softmax with 2 dimensions, where \nx = x_1 - x_2\n\n\n\n\nif the numbers in the vector are very large or very small (negative), there's an underflow / overflow problem\n\n\nsolution: subtract the max of the vector from every entry, and use the softmax on that\n\n\n\n\n\n\noften, we optimize for the log of probabilities -\n \n\\log \\text{softmax}\n\n\n\n\nuseful, because log and exp cancel each other out\n\n\ncalled \"logsoftmax\"\n\n\n\n\n\n\n\n\nPoor Conditioning\n\n\n\n\nProblem: Small change in input produces very large change in output\n\n\n\n\nGradient-Based Optimization\n\n\n\n\n\n\narg min f(x)\n: the argument \nx\n that produces the minimum of \nf(x)\n\n\n\n\nCalled the \"minimizer\"\n\n\nf(arg min f(x)) = min\n\n\n\n\n\n\nIdea: if the slope is negative, walk forwards, if it's positive, walk backwards: \"gradient descent\"\n\n\nGives us a minimum\n\n\nGradient = derivative in multiple dimensions\n\n\nIf the gradient is 0, we're either at a minimum, a maximum or a \nsaddle point\n\n\nsaddle points are common in higher dimensions\n\n\n\n\n\n\nthe goal is finding a global minimum, but we don't know if we've found it with gradient descent, except if the function is \"convex\", e.g. a quadratic function with one minimum\n\n\nThere's a derivative for every dimension\n\n\nThe gradient is a vector with all the partial derivatives for every dimension\n\n\n\n\nGradient descent for vectors\n\n\n\n\nGradient is vector that points to the steepest slope\n\n\nWe want to go in the reverse opposite of that vector to find the minimum\n\n\nanalogy: Go down a mountain by taking the steepest step each time, you'd find the valley\n\n\nminimizing: cos should be -1 -\n 180\u00b0\n\n\nThe opposite direction of \nu\n (the steepest direction)\n\n\n\n\n\n\n\n\nJacobian and Hessian Matrices\n\n\n\n\n\n\nf(x)\n is now mapping from n-dimensional to m-dimensional vector\n\n\ne.g. common between two layers in a neural network\n\n\n\n\n\n\nThe Jacobian matrix \nJ\n captures the derivate of every input entry w.r.t. to every output entry, so it's a n by m matrix\n\n\nderivates say how the change of one value affects the other\n\n\n\n\n\n\nDerivatives of derivatives can be in different directions, so there are a lot of combinations\n\n\nSecond derivative = curvature\n\n\nThe curvature of a plane is 0\n\n\nThis makes gradient descent \"easy\", just follow the derivative\n\n\n\n\n\n\nAll the combinations of curvature are captured in the Hessian matrix \nH\n\n\n\n\nThe optimal \n\\epsilon\n could be calculated with the Hessian matrix, but that's too hard to calculate (and store), so not practical\n\n\nWe can determine if we're at a minimum, maximum with the second derivative\n\n\nIf the derivate is 0, it could be a saddle point, but the test is inconclusive\n\n\nThe same concept works in higher dimensions with eigenvalues\n\n\nIf not all eigenvalues are either positive or negative, we're at a saddle point\n\n\nAlmost a chance of 1\n\n\n\n\n\n\nThe condition number tells us how much the space is \"warped\"\n\n\nif the condition number is high, gradient descent is harder (slower)", 
            "title": "Numerical Computation"
        }, 
        {
            "location": "/dl/numerical_computation/#numerical-computation", 
            "text": "Most of this is handled by frameworks", 
            "title": "Numerical Computation"
        }, 
        {
            "location": "/dl/numerical_computation/#overflow-and-underflow", 
            "text": "Underflow: So small that they're basically 0   \\log x  when x is zero:  -\\infty     softmax takes a vector, the sum will be 1, and every number will be positive -  probability distribution  sigmoid is softmax with 2 dimensions, where  x = x_1 - x_2   if the numbers in the vector are very large or very small (negative), there's an underflow / overflow problem  solution: subtract the max of the vector from every entry, and use the softmax on that    often, we optimize for the log of probabilities -   \\log \\text{softmax}   useful, because log and exp cancel each other out  called \"logsoftmax\"", 
            "title": "Overflow and Underflow"
        }, 
        {
            "location": "/dl/numerical_computation/#poor-conditioning", 
            "text": "Problem: Small change in input produces very large change in output", 
            "title": "Poor Conditioning"
        }, 
        {
            "location": "/dl/numerical_computation/#gradient-based-optimization", 
            "text": "arg min f(x) : the argument  x  that produces the minimum of  f(x)   Called the \"minimizer\"  f(arg min f(x)) = min    Idea: if the slope is negative, walk forwards, if it's positive, walk backwards: \"gradient descent\"  Gives us a minimum  Gradient = derivative in multiple dimensions  If the gradient is 0, we're either at a minimum, a maximum or a  saddle point  saddle points are common in higher dimensions    the goal is finding a global minimum, but we don't know if we've found it with gradient descent, except if the function is \"convex\", e.g. a quadratic function with one minimum  There's a derivative for every dimension  The gradient is a vector with all the partial derivatives for every dimension", 
            "title": "Gradient-Based Optimization"
        }, 
        {
            "location": "/dl/numerical_computation/#gradient-descent-for-vectors", 
            "text": "Gradient is vector that points to the steepest slope  We want to go in the reverse opposite of that vector to find the minimum  analogy: Go down a mountain by taking the steepest step each time, you'd find the valley  minimizing: cos should be -1 -  180\u00b0  The opposite direction of  u  (the steepest direction)", 
            "title": "Gradient descent for vectors"
        }, 
        {
            "location": "/dl/numerical_computation/#jacobian-and-hessian-matrices", 
            "text": "f(x)  is now mapping from n-dimensional to m-dimensional vector  e.g. common between two layers in a neural network    The Jacobian matrix  J  captures the derivate of every input entry w.r.t. to every output entry, so it's a n by m matrix  derivates say how the change of one value affects the other    Derivatives of derivatives can be in different directions, so there are a lot of combinations  Second derivative = curvature  The curvature of a plane is 0  This makes gradient descent \"easy\", just follow the derivative    All the combinations of curvature are captured in the Hessian matrix  H   The optimal  \\epsilon  could be calculated with the Hessian matrix, but that's too hard to calculate (and store), so not practical  We can determine if we're at a minimum, maximum with the second derivative  If the derivate is 0, it could be a saddle point, but the test is inconclusive  The same concept works in higher dimensions with eigenvalues  If not all eigenvalues are either positive or negative, we're at a saddle point  Almost a chance of 1    The condition number tells us how much the space is \"warped\"  if the condition number is high, gradient descent is harder (slower)", 
            "title": "Jacobian and Hessian Matrices"
        }, 
        {
            "location": "/dl/ml_basics/", 
            "text": "Machine Learning Basics\n\n\n\n\n\"Generalization\": Perform well on data it hasn't seen before\n\n\nLinear regression measures the error on the training data, not on the test data\n\n\nOnly works if training and test data sets have the same distribution\n\n\n\n\n\n\nThe \"i.i.d. assumptions\"\n\n\nAssume that all examples are independent, and training and test set are identically distributed\n\n\nUse distribution of one sample to generate all test and training samples\n\n\nOptimizing for the training data will therefore also optimize for the test data\n\n\n\n\n\n\nOverfitting: Reach a low training error, but the test error is much larger\n\n\ni.e. no generalization, only \"memorized\" the training data\n\n\nGiving it too much capacity\n\n\n\n\n\n\nCapacity: Flexibility the system has\n\n\ne.g. for linear regression, increase capacity by allowing higher-order polynomials\n\n\n\n\n\n\n\n\nCheck for systems: If you can't get your system to overfit (by giving it lots of capacity), it doesn't learn properly\n\n\n\n\n\n\nEven with a perfect model, with a reasonably complex system, there's always an error\n\n\n\n\ne.g. digital communication: noise makes it impossible to always decide correctly (between 1 and 0)\n\n\n\n\n\n\n\n\n\"No Free Lunch\" theorem: No machine learning algorithm is universally better than any other\n\n\n\n\nBut assumes data of all possible distributions\n\n\nIn ML, we always concentrate on a limited range of data\n\n\ni.e. there's no \"general\" good ML algorithm\n\n\n\n\n\n\n\n\nRegularization\n\n\n\n\n\n\n\\lambda w^T w\n: All elements squared and summed up\n\n\ni.e. small weights are preferable for low cost\n\n\n\n\n\n\nIf we pick \n\\lambda\n to be large, weights will almost be zero to achieve minimal cost (underfitting)\n\n\nAlso called a \"penalty term\"\n\n\nGoal ist to minimize the generalization error but not the training error\n\n\n\n\nHyperparameters\n\n\n\n\n\n\n\\lambda\n is a \"hyperparameter\"\n\n\nparameters that are not learned by the scheme\n\n\nto train these parameters, we use part of the training data as \"validation data\" to \"tune\" the algorithm\n\n\n\n\n\n\nAfter we've used a test set once, it is \"tainted\" and should theoretically not be used anymore\n\n\nhard in practice to have enough data\n\n\n\n\n\n\n\n\nEstimators, Bias, Variance\n\n\n\n\nEstimator: \"Sch\u00e4tzer\"\n\n\nBias: Difference between Error of \"guess\" and real data\n\n\nEstimator is unbiased: It is on average right (\"Erwartungstreu\")\n\n\nVariance of Estimator\n\n\n\"SE\": Standard Error, a.k.a \n\\sqrt{var(x)}\n\n\n\n\n\n\n\n\nThe variance of a sample is \n\\frac{\\sigma}{\\sqrt m}\n\n\n\n\ni.e. lots of data reduces the variance of the estimator\n\n\n\n\n\n\nConsistency: In the limit, the estimator \"guesses the right value\"\n\n\nVariance goes towards zero\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\n\n\n\n\n\\theta\n are free variables\n\n\nWeights of the network in a neural network\n\n\n\n\n\n\nFind \n\\theta\n such that the likelihood that the model is producing the \"correct\" values is maximized\n\n\nUsually, the log of the likelihood is maximized\n\n\nKL divergence: Measures how similar to distributions are\n\n\nGoal: minimize KL divergence\n\n\n\n\n\n\n\n\n\n\nTodo\n\n\nChapters 5.7 and 5.8 (supervised vs unsupervised learning) \n\n\n\n\nStochastic Gradient Descent\n\n\n\n\nGradient Descent with a limited sample\n\n\nNegative log-likelihood is the cost function\n\n\nMinimizing this cost = maximizing likelihood\n\n\n\n\n\n\nGradients over all of millions data points would take forever\n\n\nUse a select few of the data as an \nestimate\n of the gradient\n\n\nThis subset is called a \"minibatch\"\n\n\nHas to be chosen truly randomly from the whole data set\n\n\n\n\n\n\nlearning rate: step size for gradient descent\n\n\n\n\nBuilding a ML algorithm\n\n\n\n\nOnly gets useful once you have a lot of data", 
            "title": "Machine Learning Basics"
        }, 
        {
            "location": "/dl/ml_basics/#machine-learning-basics", 
            "text": "\"Generalization\": Perform well on data it hasn't seen before  Linear regression measures the error on the training data, not on the test data  Only works if training and test data sets have the same distribution    The \"i.i.d. assumptions\"  Assume that all examples are independent, and training and test set are identically distributed  Use distribution of one sample to generate all test and training samples  Optimizing for the training data will therefore also optimize for the test data    Overfitting: Reach a low training error, but the test error is much larger  i.e. no generalization, only \"memorized\" the training data  Giving it too much capacity    Capacity: Flexibility the system has  e.g. for linear regression, increase capacity by allowing higher-order polynomials     Check for systems: If you can't get your system to overfit (by giving it lots of capacity), it doesn't learn properly    Even with a perfect model, with a reasonably complex system, there's always an error   e.g. digital communication: noise makes it impossible to always decide correctly (between 1 and 0)     \"No Free Lunch\" theorem: No machine learning algorithm is universally better than any other   But assumes data of all possible distributions  In ML, we always concentrate on a limited range of data  i.e. there's no \"general\" good ML algorithm", 
            "title": "Machine Learning Basics"
        }, 
        {
            "location": "/dl/ml_basics/#regularization", 
            "text": "\\lambda w^T w : All elements squared and summed up  i.e. small weights are preferable for low cost    If we pick  \\lambda  to be large, weights will almost be zero to achieve minimal cost (underfitting)  Also called a \"penalty term\"  Goal ist to minimize the generalization error but not the training error", 
            "title": "Regularization"
        }, 
        {
            "location": "/dl/ml_basics/#hyperparameters", 
            "text": "\\lambda  is a \"hyperparameter\"  parameters that are not learned by the scheme  to train these parameters, we use part of the training data as \"validation data\" to \"tune\" the algorithm    After we've used a test set once, it is \"tainted\" and should theoretically not be used anymore  hard in practice to have enough data", 
            "title": "Hyperparameters"
        }, 
        {
            "location": "/dl/ml_basics/#estimators-bias-variance", 
            "text": "Estimator: \"Sch\u00e4tzer\"  Bias: Difference between Error of \"guess\" and real data  Estimator is unbiased: It is on average right (\"Erwartungstreu\")  Variance of Estimator  \"SE\": Standard Error, a.k.a  \\sqrt{var(x)}     The variance of a sample is  \\frac{\\sigma}{\\sqrt m}   i.e. lots of data reduces the variance of the estimator    Consistency: In the limit, the estimator \"guesses the right value\"  Variance goes towards zero", 
            "title": "Estimators, Bias, Variance"
        }, 
        {
            "location": "/dl/ml_basics/#maximum-likelihood-estimation", 
            "text": "\\theta  are free variables  Weights of the network in a neural network    Find  \\theta  such that the likelihood that the model is producing the \"correct\" values is maximized  Usually, the log of the likelihood is maximized  KL divergence: Measures how similar to distributions are  Goal: minimize KL divergence      Todo  Chapters 5.7 and 5.8 (supervised vs unsupervised learning)", 
            "title": "Maximum Likelihood Estimation"
        }, 
        {
            "location": "/dl/ml_basics/#stochastic-gradient-descent", 
            "text": "Gradient Descent with a limited sample  Negative log-likelihood is the cost function  Minimizing this cost = maximizing likelihood    Gradients over all of millions data points would take forever  Use a select few of the data as an  estimate  of the gradient  This subset is called a \"minibatch\"  Has to be chosen truly randomly from the whole data set    learning rate: step size for gradient descent", 
            "title": "Stochastic Gradient Descent"
        }, 
        {
            "location": "/dl/ml_basics/#building-a-ml-algorithm", 
            "text": "Only gets useful once you have a lot of data", 
            "title": "Building a ML algorithm"
        }
    ]
}